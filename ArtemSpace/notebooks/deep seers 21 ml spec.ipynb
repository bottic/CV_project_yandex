{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# КОМАНДА 21 (Deep Seers)\n",
    "### В этом ноутбуке представлено наше решение задачи классификации изображений для соревнования ML intensive Yandex Academy autumn 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВВЕДЕНИЕ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Когда мы начали работу над задачей, первым делом мы внимательно изучили предоставленный датасет. Мы загрузили данные и проанализировали их структуру, чтобы понять, какие классы представлены и как они распределены. Это позволило нам оценить, есть ли какие-либо классы, которые могут быть недопредставлены, что могло бы повлиять на качество модели.\n",
    "### Паралелльно нами были прочитаны несколько статей, котрорые определили наш первоначальный вектор развития:\n",
    "[Одна из статей](https://yfalan.github.io/files/papers/Xipeng_CVPR2018.pdf) \n",
    "> !!!!Основываясь на статье, мы использовали:\n",
    "RandomRotation для поворота изображений,  \n",
    "RandomHorizontalFlip для добавления симметричности, \n",
    "RandomErasing для случайного закрытия областей изображения, \n",
    "RandomResizedCrop для масштабирования изображений.!!!!\n",
    "### После, мы распредили обязанности: два участника команды занимались пробой различных архитектур, один занимался аугментациями для датасета.\n",
    "### Мы создали [репозиторий на github](http://github.com/bottic/CV_project_yandex) и выделили каждому участнику по воркспейсу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ХОД РАБОТЫ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Через несколько дней был готов полностью рабочий для обучения [ноутбук](https://github.com/bottic/CV_project_yandex/blob/0b79254887288665e84e9a428c5f81c171143bc7/VladSpace/project.ipynb), в нем мы имели:\n",
    "* Загрузку датасета\n",
    "* Подготовку изображений для обучения модели\n",
    "* Функции для обучения модели\n",
    "* Сама базовая модель\n",
    "* Отрисовку графиков для отслеживания успехов модели\n",
    "### Код для загрузки датасета, использовали API kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачиваем датасет\n",
    "! pip install -q kaggle # Предпологается что kaggle.json уже в этой папке\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle competitions download -c ml-intensive-yandex-academy-autumn-2024\n",
    "# Распаковка и удаление зипа\n",
    "! unzip ml-intensive-yandex-academy-autumn-2024.zip && rm ml-intensive-yandex-academy-autumn-2024.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Также был создан кастомный класс для датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPoseDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        img_dir: Папка с изображениями (В моем случае, 'drive/MyDrive/DataSets/human_poses_data/img_train').\n",
    "        csv_file: Путь к таблице с метками (например, 'train_answers.csv').\n",
    "        transform: Трансформации для предобработки изображений.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.labels = pd.read_csv(csv_file)  # Загружаем таблицу меток\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Достаем имя изображения и метку\n",
    "        img_id = self.labels.iloc[idx, 0]  # img_id (имя изображения)\n",
    "        label = self.labels.iloc[idx, 1]  # target_feature (метка)\n",
    "\n",
    "        # Загружаем изображение\n",
    "        img_path = os.path.join(self.img_dir, str(img_id)+'.jpg')\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Убедимся, что изображение в RGB\n",
    "\n",
    "        # Применяем трансформации\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для просмотра изображений из кастомного класса датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_with_labels(dataset, id_to_category, num_images=10):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 5))\n",
    "    for i in range(num_images):\n",
    "        # Достаем изображение и метку\n",
    "        image, label = dataset[i]\n",
    "\n",
    "        # Декодируем метку в категорию\n",
    "        category = id_to_category[label] if id_to_category else label\n",
    "        axes[i].imshow(image.permute(1, 2, 0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Так как мы сохраняли метрики в csv файл можно было удобно смотреть на графики обучения при помощи этого кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./train_info/logs_13_12_21_19/metrics.csv\"\n",
    "\n",
    "metrics = pd.read_csv(metrics_path)\n",
    "\n",
    "plt.figure()\n",
    "metrics[['Train Loss', 'Validation Loss', 'Validation Accuracy']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первой архитектурой, которую мы попробывали был [Alexnet](https://github.com/bottic/CV_project_yandex/blob/1f63ff09077e41349f18877b5ef845273540522f/VladSpace/project.ipynb). Мы выбрали эту модель, так как она является одной из классических архитектур для задач классификации изображений и показала хорошие результаты на различных датасетах.\n",
    "\n",
    "### Мы начали с предобученной версии AlexNet, чтобы использовать трансферное обучение и ускорить процесс обучения. Это позволило нам извлечь полезные признаки из изображений, которые были уже изучены на большом наборе данных, таком как ImageNet. Мы адаптировали модель под наш датасет, изменив последний слой, чтобы он соответствовал количеству классов в нашей задаче.\n",
    "\n",
    "### В процессе обучения мы использовали стандартные гиперпараметры, такие как скорость обучения, размер батча и количество эпох. Однако, несмотря на наши усилия, максимальная точность, которую мы смогли достичь на валидационном наборе, составила всего 0.5. Это было разочаровывающим результатом, особенно учитывая, что мы ожидали более высокую производительность от такой известной архитектуры.\n",
    "\n",
    "### Мы заметили, что начиная с 20-й эпохи, модель начала переобучаться. Это проявлялось в том, что точность на обучающем наборе продолжала расти, в то время как точность на валидационном наборе начала снижаться. Мы решили, что необходимо внести изменения в подход к обучению, чтобы улучшить обобщающую способность модели.\n",
    "\n",
    "### В качестве первых шагов по борьбе с переобучением мы начали экспериментировать с различными техниками регуляризации, такими как Dropout и L2-регуляризация.\n",
    "### Итоговая первая вразумительная версия модели выглядела так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Еще была и такая вариация модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AlexNetArtem(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlexNetArtem, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(\n",
    "                alpha=1e-4, beta=0.75, k=2, size=5\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=96, out_channels=256, kernel_size=5, padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(\n",
    "                alpha=1e-4, beta=0.75, k=2, size=5\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=256, out_channels=384, kernel_size=3, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=384, out_channels=384, kernel_size=3, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_block_5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=384, out_channels=256, kernel_size=3, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=256 * 6 * 6, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=4096, out_features=self.num_classes\n",
    "            ),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.conv_block_4(x)\n",
    "        x = self.conv_block_5(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Паралелльно мы создали модель на основе архитектуры **ResNet18**, изначально она показывала схожие с нашей первоночальной моделью, но после некоторых изменений в архитектуре, включая такие как:\n",
    "* Активационная функция Mish\n",
    "* Групповая нормализация\n",
    "* SE-блоки\n",
    "* Дропаут\n",
    "### Модель стала давать accuracy около 0.62\n",
    "*Стоит сказать, что мы обучали модели в одинаковых условиях с функцией потерь в виде кросс-энтропии, закрепленным random seed и одинаковой по количеству val и train выборке*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(nn.functional.softplus(x))\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, dropout_prob=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.mish = Mish()\n",
    "        self.group_norm1 = nn.GroupNorm(32, out_channels)\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.group_norm2 = nn.GroupNorm(32, out_channels)\n",
    "        self.dropout2 = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        self.se_block = SEBlock(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.mish(out)\n",
    "        out = self.group_norm1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.mish(out)\n",
    "        out = self.group_norm2(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = self.se_block(out)\n",
    "        out += identity\n",
    "        out = self.mish(out)\n",
    "        return out\n",
    "\n",
    "class UpdatedResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=20, dropout_prob=0.3):\n",
    "        super(UpdatedResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.mish = Mish()\n",
    "        self.group_norm = nn.GroupNorm(32, 64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], dropout_prob=dropout_prob)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dropout_prob=dropout_prob)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dropout_prob=dropout_prob)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dropout_prob=dropout_prob)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1, dropout_prob=0.3):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.GroupNorm(32, out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample, dropout_prob=dropout_prob))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels, dropout_prob=dropout_prob))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.group_norm(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def custom_resnet18(num_classes=20, dropout_prob=0.3):\n",
    "    return UpdatedResNet(ResidualBlock, [2, 2, 2, 2], num_classes, dropout_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вскоре мы сделали аугментацию данных, дабы повысить обобщающую способность модели. Датасет был раздут до 19 тыс. изображений.\n",
    "### Код для создания двух папок с аугментированными изображениями и изобажениями для валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitPFileToVal:\n",
    "    def __init__(self, path_to_dir, path_to_new_dir_val, path_to_new_dir_train, p:float):\n",
    "        \"\"\"\n",
    "        Создаёт 2 новые папки: Для валидации - % от всех изображений, для трейна - все остальное. \n",
    "        Нужно для отделения части датасета, для val данных\n",
    "\n",
    "        path_to_dir: Папка с изображениями. (img_train)\n",
    "        path_to_new_dir_val: Путь до новой папки, можно не создавать вручную.\n",
    "        path_to_new_dir_train: Путь до новой папки, можно не создавать вручную.\n",
    "        p: % изображений, которые будут отделены для валидационного датасета\n",
    "        \"\"\"\n",
    "\n",
    "        #Проверка, введеного %\n",
    "        if not isinstance(p, float):\n",
    "            raise TypeError(\"% должен быть float\")\n",
    "        if p <= 0 or p > 1:\n",
    "            raise ValueError(\"% должен быть в диапозоне: 0 < p <= 1\")\n",
    "        self.p = p\n",
    "\n",
    "        self.path_to_dir = path_to_dir\n",
    "        self.files = os.listdir(self.path_to_dir)\n",
    "        self.path_to_new_dir_val = path_to_new_dir_val\n",
    "        self.path_to_new_dir_train = path_to_new_dir_train\n",
    "\n",
    "\n",
    "\n",
    "        # Создаем папки для сохранения\n",
    "        if os.path.exists(path_to_new_dir_val):\n",
    "            shutil.rmtree(path_to_new_dir_val)  # Удаляем папку, если она уже существует\n",
    "        os.makedirs(path_to_new_dir_val, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(path_to_new_dir_train):\n",
    "            shutil.rmtree(path_to_new_dir_train)  # Удаляем папку, если она уже существует\n",
    "        os.makedirs(path_to_new_dir_train, exist_ok=True)\n",
    "\n",
    "    def split(self):\n",
    "        len_val = int(len(self.files)*self.p)\n",
    "        len_train = len(self.files) - len_val\n",
    "        with tqdm(total=len_val, desc=\"Отделение валидационных данных\", unit=\"img\") as pbar:\n",
    "            for _ in range(len_val):\n",
    "                idx = random.randint(0, len(self.files) - 1)\n",
    "                file = self.files[idx]\n",
    "                path_to_file = self.path_to_dir+f'/{file}'\n",
    "                shutil.copy(path_to_file, self.path_to_new_dir_val+f'/{file}')\n",
    "                self.files.pop(idx)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        with tqdm(total=len_train, desc=\"Отделение тренировочных данных\", unit=\"img\") as pbar:\n",
    "            for _ in range(len_train):\n",
    "                idx = random.randint(0, len(self.files) - 1)\n",
    "                file = self.files[idx]\n",
    "                path_to_file = self.path_to_dir+f'/{file}'\n",
    "                shutil.copy(path_to_file, self.path_to_new_dir_train+f'/{file}')\n",
    "                self.files.pop(idx)\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"\\nОтделено {len_val} изображений, для валидации в папку: {self.path_to_new_dir_val}\")\n",
    "        print(f\"Отделено {len_train} изображений, для тренировки в папку: {self.path_to_new_dir_train}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDatasetSaver:\n",
    "    def __init__(self, original_train_dataset, csv_path, augmentation_transform, new_train_dataset_path, output_csv_path, augment_count):\n",
    "        \"\"\"\n",
    "        Создает и сохраняет аугментированный датасет из тренировочного.\n",
    "        Также создает csv с ответами на аугментированный датасет, сохраняет ответы из train_answers\n",
    "\n",
    "        original_train_dataset: Исходный датасет (torch Dataset).\n",
    "        csv_path: путь к csv с ответами\n",
    "        augmentation_transform: Трансформации для аугментации.\n",
    "        new_train_dataset_path: Папка, куда будут сохранены данные.\n",
    "        output_csv_path: Путь, куда будет сохранена csv\n",
    "        augment_count: Количество аугментированных изображений, которые нужно создать.\n",
    "        \"\"\"\n",
    "        self.original_train_dataset_path = original_train_dataset\n",
    "        self.augmentation_transform = augmentation_transform\n",
    "        self.new_train_dataset_path = new_train_dataset_path\n",
    "        self.output_csv_path = output_csv_path\n",
    "        self.original_labels = pd.read_csv(csv_path)\n",
    "\n",
    "        \n",
    "        if augment_count >= len(original_train_dataset):\n",
    "            print(\"Введеный augment_count > длины датасета\")\n",
    "        self.augment_count = augment_count\n",
    "\n",
    "\n",
    "        # Создаем папку для сохранения\n",
    "        if os.path.exists(new_train_dataset_path):\n",
    "            shutil.rmtree(new_train_dataset_path)  # Удаляем папку, если она уже существует\n",
    "        os.makedirs(new_train_dataset_path, exist_ok=True)\n",
    "\n",
    "    def save(self):\n",
    "        new_entries = []\n",
    "        max_id = self.original_labels['img_id'].max()\n",
    "        with tqdm(total=self.augment_count, desc=\"Создание аугментированных данных\", unit=\"img\") as pbar:\n",
    "            for i in range(self.augment_count):\n",
    "                # Выбираем случайное изображение из оригинального датасета\n",
    "                image, label, img_id = random.choice(self.original_train_dataset_path)\n",
    "\n",
    "                # Преобразуем в тензор, если это PIL.Image\n",
    "                if not isinstance(image, Image.Image):\n",
    "                    raise ValueError(f\"Unsupported image format: {type(image)}\")\n",
    "\n",
    "                # Применяем аугментацию\n",
    "                augmented_image = self.augmentation_transform(image)\n",
    "\n",
    "                \n",
    "\n",
    "                # Генерируем имя файла, сохраняя оригинальное имя с префиксом\n",
    "                img_id = max_id+i+1\n",
    "\n",
    "                new_entries.append({'img_id': int(img_id), 'target_feature': label})\n",
    "\n",
    "                img_save_path = os.path.join(self.new_train_dataset_path, f\"{img_id}.jpg\")\n",
    "\n",
    "                # Преобразуем аугментированный тензор обратно в PIL.Image и сохраняем\n",
    "                augmented_image = transforms.ToPILImage()(augmented_image)  # Перевод тензора в PIL\n",
    "                augmented_image.save(img_save_path)\n",
    "\n",
    "                pbar.update(1)\n",
    "        # Создание DataFrame для новых данных\n",
    "        augmented_df = pd.DataFrame(new_entries)\n",
    "        combined_df = pd.concat([self.original_labels, augmented_df], ignore_index=True)\n",
    "        combined_df.to_csv(self.output_csv_path, index=False)\n",
    "        \n",
    "        print(f\"CSV файл с метками успешно сохранён в: {self.output_csv_path}\")\n",
    "        print(f\"\\nСохранено {self.augment_count} аугментированных изображений в папке: {self.new_train_dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_folders(path_train_dataset, path_augmented_only_dataset, path_destination_folder):\n",
    "    \"\"\"\n",
    "    Объединяет две папки с картинками в одну. УДАЛЯЕТ папку path_augmented_only_dataset\n",
    "\n",
    "    :param path_train_dataset: Путь к первой исходной папке\n",
    "    :param path_augmented_only_dataset: Путь ко второй исходной папке\n",
    "    :param path_destination_folder: Путь к папке назначения\n",
    "    \"\"\"\n",
    "    # Создаем папку назначения, если она не существует\n",
    "    if os.path.exists(path_destination_folder):\n",
    "        shutil.rmtree(path_destination_folder)  # Удаляем папку, если она уже существует\n",
    "    os.makedirs(path_destination_folder, exist_ok=True)\n",
    "    \n",
    "    # Функция для копирования файлов из папки\n",
    "    def copy_files_from_folder(folder):\n",
    "        with tqdm(total=len(os.listdir(folder)), desc=f\"Копирование из {folder}\", unit=\"img\") as pbar:\n",
    "            for file_name in os.listdir(folder):\n",
    "                source_path = os.path.join(folder, file_name)\n",
    "                dest_path = os.path.join(path_destination_folder, file_name)\n",
    "                \n",
    "                # Проверяем, является ли элемент файлом\n",
    "                if os.path.isfile(source_path):\n",
    "                    if os.path.exists(dest_path):\n",
    "                        raise KeyError('файл с таким именем уже существует в папке назначения')\n",
    "                    shutil.copy2(source_path, dest_path)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Копируем файлы из обеих папок\n",
    "    copy_files_from_folder(path_train_dataset)\n",
    "    copy_files_from_folder(path_augmented_only_dataset)\n",
    "    #Удаляем папку с аугментацией\n",
    "    shutil.rmtree(path_augmented_only_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По итогу получилось:   \n",
    "папка img_train -- картинки в оригинале,  \n",
    "папка train_dataset -- часть датасета, для обучения,  \n",
    "папка val_dataset -- часть датасета, для валидации,   \n",
    "папка augmented_and_train_dataset -- augmented_only_dataset+img_train,   \n",
    "augmented_train_answers.csv -- ответы на все папки  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Всё тот же кастомный резнет стал давать accuracy в целых 0.7 на 50 эпохе!\n",
    "### Это значило, что пришло время первого сабмита, но тут нас ждало потресение. F1-score на каггле показал значение в 0.32411.\n",
    "### Причин для такого могло быть много, от переобучения модели для val, до банального дисбаланса классов. В любом случае было решено исправить метрику для обучения на weighted F1-score, чтобы не повторять таких ситуций и не заходить в тупик\n",
    "### Мы продолжали углубляться в вопрос различных архитектур. Была замечена интересная закономерность, при которой модели вплоть до 4 млн (в нашем **ResNet18** было 11.2 млн) параметров показывают скоры на порядок выше.\n",
    "### В итоге мы протестировали архитектуры: **MobileNetV2**(3504872 параметров), **RegNetX16GF**(9190136 параметров), **RegNetY400mf**(4344144 параметров) и многие другие...\n",
    "*Стоит подметить, что естественно все модели были **not pretrained***\n",
    "### Настоящим золотым граалем среди архитектур оказался **EfficentNet**, помимо его выделяющегося F1-score со значением в 0.6 в базовой комплектации B0, он имел всего 4млн параметров, что давало гигансткие возможности для кастомизации. Мы так же попробовали и другие модели семейства, но их результат в связи с большим количеством параметров, подтвердив нашу теорию, оказался хуже.\n",
    "### Чтобы понять, почему именно **EfficentNet** работает лучше и в дальнейшем маштабировать его, мы разобрали его архитектуру и выделили следующие особенности:\n",
    "* SE-модули\n",
    "* Эффективные MBConv-блоки с Depthwise и Pointwise свёртками\n",
    "* Возможность легкого маштабирования с помощью Compound Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        self.base_model = efficientnet_b0(pretrained=False)\n",
    "\n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features, num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "def custom_efficientnet_b0(num_classes=20):\n",
    "    return CustomEfficientNet(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данную модель мы стали использовать в качестве основной, первый же сабмит на каггл выдал F1 0.62395\n",
    "### Один из участников стал искать способы повысить точность модель не меняя ее архитектуру, были опробованы:\n",
    "* Различные виды loss-функций(Focal Loss, Cross-Entropy Loss...)\n",
    "* Различные гиперпараметры(weight decay, learning rate...)\n",
    "* Изменения аугментаций для датасета\n",
    "* Различное количество бачей\n",
    "### В то же время мы занимались кастомизацией самой модели. После активного \"ковыряния\" в архитектуре стало понятно, что внутренности модели лучше оставить в покое. Наращивание ее новыми блоками стало основным вектором дальнейшего развития. Мы добавляли свертки, skip-connectinos, даже внедряли Vision Transformer Layer. Итогом этого всего стала данная модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=False)\n",
    "        self.extra_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.base_model.features[0][0] = nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extra_conv(x)\n",
    "        return self.base_model(x)\n",
    "\n",
    "def custom_efficientnet_b0(num_classes=20):\n",
    "    return CustomEfficientNet(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Три дополнительных сверточных слоя с фильтрами размером 3×3, BatchNorm и ReLU добавляют способность модели извлекать более сложные локальные признаки на раннем этапе, Identity mapping позволяет усилить нелинейные преобразования. Основной фишкой этой модели стал высокий Dropout, он позволил нам иметь рост по скору на высоких эпохах, хоть и не всегда стабильный. Такая модель давала стабильно увеличивающийся скор вплоть до 0.65. При этом ее вариации по весам с разницей в 15-20 эпох имели около 1700 различных target_feature в сабмишнах. В теории это дает нам возможность ансамблировать несколько моделей и значительно увеличить скор.\n",
    "### Нашим итоговым решение стал ансамбль из двух кастомных моделей на основе **ResNet18**, и модель на основе **EfficentNet-B0** на весах с разных эпох.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ИТОГОВОЕ РЕШЕНИЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала объявим модели, которые будут участвовать в ансамбле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(nn.functional.softplus(x))\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, dropout_prob=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.mish = Mish()\n",
    "        self.group_norm1 = nn.GroupNorm(32, out_channels)\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.group_norm2 = nn.GroupNorm(32, out_channels)\n",
    "        self.dropout2 = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        self.se_block = SEBlock(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.mish(out)\n",
    "        out = self.group_norm1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.mish(out)\n",
    "        out = self.group_norm2(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = self.se_block(out)\n",
    "        out += identity\n",
    "        out = self.mish(out)\n",
    "        return out\n",
    "\n",
    "class UpdatedResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=20, dropout_prob=0.3):\n",
    "        super(UpdatedResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.mish = Mish()\n",
    "        self.group_norm = nn.GroupNorm(32, 64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], dropout_prob=dropout_prob)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dropout_prob=dropout_prob)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dropout_prob=dropout_prob)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dropout_prob=dropout_prob)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1, dropout_prob=0.3):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.GroupNorm(32, out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample, dropout_prob=dropout_prob))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels, dropout_prob=dropout_prob))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.group_norm(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def custom_resnet18(num_classes=20, dropout_prob=0.3):\n",
    "    return UpdatedResNet(ResidualBlock, [2, 2, 2, 2], num_classes, dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res = custom_resnet18(dropout_prob=0.5).to(device)\n",
    "model_res_2 = custom_resnet18(dropout_prob=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        # Load the base EfficientNet-B0 architecture without pretrained weights\n",
    "        self.base_model = efficientnet_b0(pretrained=False)\n",
    "\n",
    "        # Replace the last classifier layer for 20 classes\n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features, num_classes, bias=True)  # Explicitly set bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "def custom_efficientnet_b0(num_classes=20):\n",
    "    return CustomEfficientNet(num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effnb0 = custom_efficientnet_b0(num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "\n",
    "        # Загрузка базовой модели EfficientNet-B0 без предобученных весов\n",
    "        self.base_model = efficientnet_b0(pretrained=False)\n",
    "\n",
    "        # Добавление дополнительных сверточных слоев и Batch Normalization\n",
    "        # Сначала добавляем дополнительные сверточные слои и Batch Normalization\n",
    "        self.extra_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1),  # Identity mapping\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Обновляем первый слой базовой модели для использования 64 каналов (после добавленных слоев)\n",
    "        self.base_model.features[0][0] = nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Заменяем последний классификатор для 20 классов\n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=True),  # Увеличиваем dropout\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Применяем дополнительные слои перед базовой моделью\n",
    "        x = self.extra_conv(x)\n",
    "        \n",
    "        # Дальше передаем результат в базовую модель\n",
    "        return self.base_model(x)\n",
    "\n",
    "def custom_efficientnet_b0(num_classes=20):\n",
    "    return CustomEfficientNet(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom_effn_401 = custom_efficientnet_b0(num_classes=20)\n",
    "model_custom_effn_300 = custom_efficientnet_b0(num_classes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После объявления всех моделей загружаем веса (Лежат на [google drive](https://drive.google.com/drive/folders/18pqO6QCQjDEGPZ28Z5VcWkbDqmdSGKe6?usp=sharing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_custom_effn_401 = './train_info/checkpoints_17_12_19_24/best_checkpoint_251_val_F1=0.6286.pt'\n",
    "path_custom_effn_300 = './train_info/checkpoints_17_12_19_24/best_checkpoint_142_val_F1=0.6188.pt'\n",
    "path_res_2 = './train_info/checkpoints_11_12_7_1/best_checkpoint_40_val_accuracy=0.6990.pt'\n",
    "path_effnb0 = './train_info/check_effnb0/best_checkpoint_18_val_F1=0.5903.pt'\n",
    "path_res_check = './train_info/checkpoints_14_12_17_44/best_UpdatedResNet_checkpoint_22_val_accuracy=0.6505.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_custom_effn_401 = torch.load(path_custom_effn_401, map_location=torch.device(device))\n",
    "checkpoint_custom_effn_300 = torch.load(path_custom_effn_300, map_location=torch.device(device))\n",
    "checkpoint_res_2  = torch.load(path_res_2, map_location=torch.device(device))\n",
    "checkpoint_effnb0  = torch.load(path_effnb0, map_location=torch.device(device))\n",
    "checkpoint_res = torch.load(path_res_check, map_location=torch.device(device))\n",
    "\n",
    "model_custom_effn_401.load_state_dict(checkpoint_custom_effn_401[\"model\"])\n",
    "model_custom_effn_300.load_state_dict(checkpoint_custom_effn_300[\"model\"])\n",
    "model_res_2.load_state_dict(checkpoint_res_2[\"model\"])\n",
    "model_effnb0.load_state_dict(checkpoint_effnb0[\"model\"])\n",
    "model_res.load_state_dict(checkpoint_res[\"model\"])\n",
    "\n",
    "model_custom_effn_401.eval()\n",
    "model_custom_effn_300.eval()\n",
    "model_res_2.eval()\n",
    "model_effnb0.eval()\n",
    "model_res.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Усредненный ансамбль\n",
    "def ensemble_predict(dataloader, model_1, model_2, model_3, model_4,model_5, device='cuda'):\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "    model_3.to(device)\n",
    "    model_4.to(device)\n",
    "    model_5.to(device)\n",
    "\n",
    "    predictions = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, targets, _ = batch  # inputs - изображения, targets - метки\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Получаем вероятности от обеих моделей\n",
    "            probs_1 = F.softmax(model_1(inputs), dim=1)\n",
    "            probs_2 = F.softmax(model_2(inputs), dim=1)\n",
    "            probs_3 = F.softmax(model_3(inputs), dim=1)\n",
    "            probs_4 = F.softmax(model_4(inputs), dim=1)\n",
    "            probs_5 = F.softmax(model_5(inputs), dim=1)\n",
    "\n",
    "            # Усредняем вероятности\n",
    "            ensemble_probs = (probs_1 + probs_2 + probs_3 + probs_4+ probs_5) / 5\n",
    "\n",
    "            # Выбираем класс с максимальной вероятностью\n",
    "            predicted_classes = torch.argmax(ensemble_probs, dim=1)\n",
    "            predictions.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "            # Сохраняем изображения и метки для визуализации\n",
    "            images.extend(inputs.cpu())\n",
    "            labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    return predictions, images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "\n",
    "\n",
    "def visualize_predictions(images, labels, predictions, class_names,  num_images= 9):\n",
    "    \"\"\"\n",
    "    Визуализирует изображения с реальными и предсказанными метками.\n",
    "\n",
    "    Args:\n",
    "        images (list): Список изображений.\n",
    "        labels (list): Реальные метки.\n",
    "        predictions (list): Предсказанные метки.\n",
    "        class_names (dict): Словарь с именами классов, где ключ - метка, значение - имя класса.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(int(sqrt(num_images)), int(sqrt(num_images)), i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0))  # Преобразуем изображение для отображения\n",
    "        plt.title(f\"Real: {class_names[labels[i]]}\\nPred: {class_names[predictions[i]]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1(predictions, labels):\n",
    "    \"\"\"\n",
    "    Вычисляет F1-меру для предсказаний ансамбля.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): Список предсказанных классов.\n",
    "        labels (list): Список реальных меток.\n",
    "\n",
    "    Returns:\n",
    "        float: Значение F1-меры.\n",
    "    \"\"\"\n",
    "    return f1_score(labels, predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоняем через датасет через ансамбль и высчитываем f1, он не равен тому, что на kaggle, но отражает ситуацию, что позволяет не тратить лишине сабмиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions, images, labels = ensemble_predict(val_loader, model_custom_effn_401, model_custom_effn_300, model_effnb0,model_res_2, model_res)\n",
    "f1 = calculate_f1(ensemble_predictions, labels)\n",
    "print(f\"f1: {f1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код, для создания csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path = \"../data/img_test\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_image_files = [f for f in os.listdir(test_images_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "results = []\n",
    "\n",
    "model_1 = model_custom_effn_401\n",
    "model_2 = model_custom_effn_300\n",
    "model_3 = model_effnb0\n",
    "model_4 = model_res\n",
    "model_5 = model_res_2\n",
    "# model_6 = model_custom_effn_300\n",
    "\n",
    "for image_file in test_image_files:\n",
    "    image_path = os.path.join(test_images_path, image_file)\n",
    "    image = Image.open(image_path).convert(\"RGB\") \n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs_1 = F.softmax(model_1(image), dim=1)\n",
    "        probs_2 = F.softmax(model_2(image), dim=1)\n",
    "        probs_3 = F.softmax(model_3(image), dim=1)\n",
    "        probs_4 = F.softmax(model_4(image), dim=1)\n",
    "        probs_5 = F.softmax(model_5(image), dim=1)\n",
    "        # probs_6 = F.softmax(model_6(image), dim=1)\n",
    "\n",
    "        # Усредняем вероятности\n",
    "        # ensemble_probs = (probs_1 + probs_2 + probs_3 + probs_4+probs_5+probs_6) / 6\n",
    "        ensemble_probs = (probs_1 + probs_2 + probs_3 + probs_4+probs_5) / 5\n",
    "\n",
    "        # Находим класс с максимальной вероятностью\n",
    "        predicted_class = torch.argmax(ensemble_probs, dim=1).item()\n",
    "\n",
    "    image_id = os.path.splitext(image_file)[0]\n",
    "    results.append({\"id\": image_id, \"target_feature\": predicted_class})\n",
    "\n",
    "submission_df = pd.DataFrame(results)\n",
    "\n",
    "submission_file_path = \"./submission.csv\"\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Submission file saved to: {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВЫВОДЫ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### Возможности развития данной модели - очень велики. Мы несомненно получили один из лучших скоров на курсе с моделью, которая имеет всего 4074512 параметров. Очевидно и то, что ее маштабирование в условиях реальной задачи представляется во много раз более легким, чем того же ResNet18.\n",
    "### Итого на эту работу у нас ушло около трех лимитов Kaggle по GPU, бесчисленное количество ночного обучения на локальных мощностях и 60ГБ на диске для весов моделей. Мы рады, что смогли показать достойный результат, несмотря на все обстоятельства."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
